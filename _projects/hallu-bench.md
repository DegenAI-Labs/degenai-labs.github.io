---
layout: project
title: "Hallu-Bench"
slug: hallu-bench
status: active
since: 2026-01
tags: [evaluation, hallucination, benchmarks]
---

A unified â€œworld-modelâ€ definition of hallucination that makes sources of truth explicit and enables scalable benchmarks in fully-specified environments.

## Motivations
Hallucination has become an overloaded term across translation, summarization, open-domain QA, retrieval-augmented generation, multimodal models, and agentic systemsâ€”often meaning different things depending on the task. This fragmentation makes it hard to compare benchmarks, interpret mitigation results, or even agree on what counts as a hallucination versus a planning or instruction-following failure. Our motivation is to put these scattered notions on a single foundation by making the implicit assumptions explicit: what the system treats as â€œtruth,â€ what evidence it is allowed to use, and how conflicts between sources should be resolved.

## Overview
We propose a unified definition of hallucination as inaccurate internal world modeling that becomes observable through a modelâ€™s outputsâ€”formalized as the production of at least one false atomic claim under a specified reference world. Concretely, any hallucination judgment is parameterized by (i) a reference world model ğ‘Š (the source of truth, e.g., a document, knowledge base, or environment state), (ii) a view function ğ‘‰ (what information is visible/available to the model for a given input), and (iii) a conflict policy ğ‘ƒ and resulting truth function ğ‘‡ (how contradictions are resolved and claims are labeled true/false/unknown). Under this lens, many prior definitions become special cases that differ mainly in their choices of ğ‘Š, ğ‘‰, and ğ‘ƒ.

Building on the formalism, we outline a path toward a family of scalable hallucination benchmarks grounded in synthetic or simulator-defined environments where ğ‘Š is fully specified and truth labels can be computed by construction. This enables controlled stress tests across difficulty, partial observability, long interaction histories, and conflicting evidenceâ€”while cleanly separating hallucinations (belief/world-model errors) from other failure modes like poor planning or incentive-driven overconfidence.
